# CVPR 2024 è®ºæ–‡å’Œå¼€æºé¡¹ç›®åˆé›†(Papers with Code)

CVPR 2024 decisions are now available on OpenReviewï¼

> æ³¨0ï¼šé¡¹ç›®æ¥è‡ªäº https://github.com/amusi/CVPR2024-Papers-with-Codeï¼Œ å½“å‰é¡¹ç›®å°†åŸæ–‡é‡Œçš„æ ‡é¢˜ç”¨ç¿»è¯‘å·¥å…·è½¬ä¸ºä¸­æ–‡ï¼Œæœªåšä¿®è®¢ï¼Œä»…ä½œå‚è€ƒ

> æ³¨1ï¼šæ¬¢è¿å„ä½å¤§ä½¬æäº¤issueï¼Œåˆ†äº«CVPR 2024è®ºæ–‡å’Œå¼€æºé¡¹ç›®ï¼
>
> æ³¨2ï¼šå…³äºå¾€å¹´CVé¡¶ä¼šè®ºæ–‡ä»¥åŠå…¶ä»–ä¼˜è´¨CVè®ºæ–‡å’Œå¤§ç›˜ç‚¹ï¼Œè¯¦è§ï¼š https://github.com/amusi/daily-paper-computer-vision
>
> - [CVPR 2019](CVPR2019-Papers-with-Code.md)
> - [CVPR 2020](CVPR2020-Papers-with-Code.md)
> - [CVPR 2021](CVPR2021-Papers-with-Code.md)
> - [CVPR 2022](CVPR2022-Papers-with-Code.md)
> - [CVPR 2023](CVPR2022-Papers-with-Code.md)

æ¬¢è¿æ‰«ç åŠ å…¥ã€CVerå­¦æœ¯äº¤æµç¾¤ã€‘ï¼Œè¿™æ˜¯æœ€å¤§çš„è®¡ç®—æœºè§†è§‰AIçŸ¥è¯†æ˜Ÿçƒï¼æ¯æ—¥æ›´æ–°ï¼Œç¬¬ä¸€æ—¶é—´åˆ†äº«æœ€æ–°æœ€å‰æ²¿çš„è®¡ç®—æœºè§†è§‰ã€AIç»˜ç”»ã€å›¾åƒå¤„ç†ã€æ·±åº¦å­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒå’ŒAIGCç­‰æ–¹å‘çš„å­¦ä¹ èµ„æ–™ï¼Œå­¦èµ·æ¥ï¼

![](CVerå­¦æœ¯äº¤æµç¾¤.png)

# ã€CVPR 2024 è®ºæ–‡å¼€æºç›®å½•ã€‘

- [3DGS(Gaussian Splatting)](#3DGS)
- [Avatars](#Avatars)
- [Backbone](#Backbone)
- [CLIP](#CLIP)
- [MAE](#MAE)
- [Embodied AI](#Embodied-AI)
- [GAN](#GAN)
- [GNN](#GNN)
- [å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)](#MLLM)
- [å¤§è¯­è¨€æ¨¡å‹(LLM)](#LLM)
- [NAS](#NAS)
- [OCR](#OCR)
- [NeRF](#NeRF)
- [DETR](#DETR)
- [Prompt](#Prompt)
- [æ‰©æ•£æ¨¡å‹(Diffusion Models)](#Diffusion)
- [ReID(é‡è¯†åˆ«)](#ReID)
- [é•¿å°¾åˆ†å¸ƒ(Long-Tail)](#Long-Tail)
- [Vision Transformer](#Vision-Transformer)
- [è§†è§‰å’Œè¯­è¨€(Vision-Language)](#VL)
- [è‡ªç›‘ç£å­¦ä¹ (Self-supervised Learning)](#SSL)
- [æ•°æ®å¢å¼º(Data Augmentation)](#DA)
- [ç›®æ ‡æ£€æµ‹(Object Detection)](#Object-Detection)
- [å¼‚å¸¸æ£€æµ‹(Anomaly Detection)](#Anomaly-Detection)
- [ç›®æ ‡è·Ÿè¸ª(Visual Tracking)](#VT)
- [è¯­ä¹‰åˆ†å‰²(Semantic Segmentation)](#Semantic-Segmentation)
- [å®ä¾‹åˆ†å‰²(Instance Segmentation)](#Instance-Segmentation)
- [å…¨æ™¯åˆ†å‰²(Panoptic Segmentation)](#Panoptic-Segmentation)
- [åŒ»å­¦å›¾åƒ(Medical Image)](#MI)
- [åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation)](#MIS)
- [è§†é¢‘ç›®æ ‡åˆ†å‰²(Video Object Segmentation)](#VOS)
- [è§†é¢‘å®ä¾‹åˆ†å‰²(Video Instance Segmentation)](#VIS)
- [å‚è€ƒå›¾åƒåˆ†å‰²(Referring Image Segmentation)](#RIS)
- [å›¾åƒæŠ å›¾(Image Matting)](#Matting)
- [å›¾åƒç¼–è¾‘(Image Editing)](#Image-Editing)
- [Low-level Vision](#LLV)
- [è¶…åˆ†è¾¨ç‡(Super-Resolution)](#SR)
- [å»å™ª(Denoising)](#Denoising)
- [å»æ¨¡ç³Š(Deblur)](#Deblur)
- [è‡ªåŠ¨é©¾é©¶(Autonomous Driving)](#Autonomous-Driving)
- [3Dç‚¹äº‘(3D Point Cloud)](#3D-Point-Cloud)
- [3Dç›®æ ‡æ£€æµ‹(3D Object Detection)](#3DOD)
- [3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation)](#3DSS)
- [3Dç›®æ ‡è·Ÿè¸ª(3D Object Tracking)](#3D-Object-Tracking)
- [3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion)](#3DSSC)
- [3Dé…å‡†(3D Registration)](#3D-Registration)
- [3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation)](#3D-Human-Pose-Estimation)
- [3Däººä½“Meshä¼°è®¡(3D Human Mesh Estimation)](#3D-Human-Pose-Estimation)
- [åŒ»å­¦å›¾åƒ(Medical Image)](#Medical-Image)
- [å›¾åƒç”Ÿæˆ(Image Generation)](#Image-Generation)
- [è§†é¢‘ç”Ÿæˆ(Video Generation)](#Video-Generation)
- [3Dç”Ÿæˆ(3D Generation)](#3D-Generation)
- [è§†é¢‘ç†è§£(Video Understanding)](#Video-Understanding)
- [è¡Œä¸ºæ£€æµ‹(Action Detection)](#Action-Detection)
- [æ–‡æœ¬æ£€æµ‹(Text Detection)](#Text-Detection)
- [çŸ¥è¯†è’¸é¦(Knowledge Distillation)](#KD)
- [æ¨¡å‹å‰ªæ(Model Pruning)](#Pruning)
- [å›¾åƒå‹ç¼©(Image Compression)](#IC)
- [ä¸‰ç»´é‡å»º(3D Reconstruction)](#3D-Reconstruction)
- [æ·±åº¦ä¼°è®¡(Depth Estimation)](#Depth-Estimation)
- [è½¨è¿¹é¢„æµ‹(Trajectory Prediction)](#TP)
- [è½¦é“çº¿æ£€æµ‹(Lane Detection)](#Lane-Detection)
- [å›¾åƒæè¿°(Image Captioning)](#Image-Captioning)
- [è§†è§‰é—®ç­”(Visual Question Answering)](#VQA)
- [æ‰‹è¯­è¯†åˆ«(Sign Language Recognition)](#SLR)
- [è§†é¢‘é¢„æµ‹(Video Prediction)](#Video-Prediction)
- [æ–°è§†ç‚¹åˆæˆ(Novel View Synthesis)](#NVS)
- [Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ )](#ZSL)
- [ç«‹ä½“åŒ¹é…(Stereo Matching)](#Stereo-Matching)
- [ç‰¹å¾åŒ¹é…(Feature Matching)](#Feature-Matching)
- [åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation)](#SGG)
- [éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations)](#INR)
- [å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment)](#IQA)
- [è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment)](#Video-Quality-Assessment)
- [æ•°æ®é›†(Datasets)](#Datasets)
- [æ–°ä»»åŠ¡(New Tasks)](#New-Tasks)
- [å…¶ä»–(Others)](#Others)

<a name="3DGS"></a>

# 3DGS(Gaussian Splatting)

**Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering**
**Scaffold-GSï¼šç»“æ„åŒ–3Dé«˜æ–¯å‡½æ•°ï¼Œç”¨äºè§†å›¾è‡ªé€‚åº”æ¸²æŸ“**

- Homepage: https://city-super.github.io/scaffold-gs/
- Paper: https://arxiv.org/abs/2312.00109
- Code: https://github.com/city-super/Scaffold-GS

**GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis**
**GPS-Gaussianï¼šå¯æ³›åŒ–çš„åƒç´ çº§3Dé«˜æ–¯åˆ†å±‚æŠ€æœ¯ï¼Œç”¨äºå®æ—¶ç”Ÿæˆäººç±»æ–°é¢–è§†è§’åˆæˆ**

- Homepage: https://shunyuanzheng.github.io/GPS-Gaussian 
- Paper: https://arxiv.org/abs/2312.02155
- Code: https://github.com/ShunyuanZheng/GPS-Gaussian

**GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**
**é«˜æ–¯å¤´åƒï¼šé€šè¿‡å¯åŠ¨3Dé«˜æ–¯å®ç°ä»å•ä¸ªè§†é¢‘ä¸­ç”Ÿæˆé€¼çœŸçš„äººç±»å¤´åƒå»ºæ¨¡**

- Paper: https://arxiv.org/abs/2312.02134
- Code: https://github.com/huliangxiao/GaussianAvatar

**GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting**
**é«˜æ–¯ç¼–è¾‘å™¨ï¼šåˆ©ç”¨é«˜æ–¯å–·æº…æŠ€æœ¯å®ç°å¿«é€Ÿå¯æ§çš„3Dç¼–è¾‘**

- Paper: https://arxiv.org/abs/2311.14521
- Code: https://github.com/buaacyw/GaussianEditor 

**Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction**
**å¯å˜å½¢3Dé«˜æ–¯å‡½æ•°ç”¨äºé«˜ä¿çœŸå•ç›®åŠ¨æ€åœºæ™¯é‡å»º**

- Homepage: https://ingra14m.github.io/Deformable-Gaussians/ 
- Paper: https://arxiv.org/abs/2309.13101
- Code: https://github.com/ingra14m/Deformable-3D-Gaussians

**SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes**
**SC-GSï¼šç”¨äºå¯ç¼–è¾‘åŠ¨æ€åœºæ™¯çš„ç¨€ç–æ§åˆ¶é«˜æ–¯å–·æº…**

- Homepage: https://yihua7.github.io/SC-GS-web/ 
- Paper: https://arxiv.org/abs/2312.14937
- Code: https://github.com/yihua7/SC-GS

**Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis**
**æ—¶ç©ºé«˜æ–¯ç‰¹å¾å–·æº…æŠ€æœ¯ç”¨äºå®æ—¶åŠ¨æ€è§†å›¾åˆæˆ**

- Homepage: https://oppo-us-research.github.io/SpacetimeGaussians-website/ 
- Paper: https://arxiv.org/abs/2312.16812
- Code: https://github.com/oppo-us-research/SpacetimeGaussians

**DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization**
**DNGaussianï¼šé€šè¿‡å…¨å±€-å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ä¼˜åŒ–ç¨€ç–è§†å›¾3Dé«˜æ–¯è¾å°„åœº**

- Homepage: https://fictionarry.github.io/DNGaussian/
- Paper: https://arxiv.org/abs/2403.06912
- Code: https://github.com/Fictionarry/DNGaussian

**4D Gaussian Splatting for Real-Time Dynamic Scene Rendering**
**å®æ—¶åŠ¨æ€åœºæ™¯æ¸²æŸ“çš„4Dé«˜æ–¯æ•£æ–‘æŠ€æœ¯**

- Paper: https://arxiv.org/abs/2310.08528
- Code: https://github.com/hustvl/4DGaussians

**GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models**
**é«˜æ–¯æ¢¦è€…ï¼šé€šè¿‡è¿æ¥äºŒç»´å’Œä¸‰ç»´æ‰©æ•£æ¨¡å‹å®ç°ä»æ–‡æœ¬åˆ°3Dé«˜æ–¯çš„å¿«é€Ÿç”Ÿæˆ**

- Paper: https://arxiv.org/abs/2310.08529
- Code: https://github.com/hustvl/GaussianDreamer

<a name="Avatars"></a>

# Avatars

**GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**
**é«˜æ–¯å¤´åƒï¼šé€šè¿‡å¯åŠ¨ç”»çš„3Dé«˜æ–¯å®ç°ä»å•ä¸ªè§†é¢‘åˆ°é€¼çœŸçš„äººåƒå»ºæ¨¡**

- Paper: https://arxiv.org/abs/2312.02134
- Code: https://github.com/huliangxiao/GaussianAvatar

**Real-Time Simulated Avatar from Head-Mounted Sensors**
**å®æ—¶æ¨¡æ‹Ÿå¤´éƒ¨ä½©æˆ´ä¼ æ„Ÿå™¨ç”Ÿæˆçš„è™šæ‹Ÿå½¢è±¡**

- Homepage: https://www.zhengyiluo.com/SimXR/
- Paper: https://arxiv.org/abs/2403.06862

<a name="Backbone"></a>

# Backbone

**RepViT: Revisiting Mobile CNN From ViT Perspective**
**RepViTï¼šä»ViTè§†è§’é‡æ–°å®¡è§†ç§»åŠ¨CNN**

- Paper: https://arxiv.org/abs/2307.09283
- Code: https://github.com/THU-MIG/RepViT

**TransNeXt: Robust Foveal Visual Perception for Vision Transformers**
**TransNeXtï¼šé’ˆå¯¹è§†è§‰Transformerçš„é²æ£’æ€§é»„æ–‘è§†è§‰æ„ŸçŸ¥**

- Paper: https://arxiv.org/abs/2311.17132
- Code: https://github.com/DaiShiResearch/TransNeXt

<a name="CLIP"></a>

# CLIP

**Alpha-CLIP: A CLIP Model Focusing on Wherever You Want**
**Alpha-CLIPï¼šä¸€ä¸ªèšç„¦äºæ‚¨æ‰€æƒ³ä¹‹å¤„çš„CLIPæ¨¡å‹**

- Paper: https://arxiv.org/abs/2312.03818
- Code: https://github.com/SunzeY/AlphaCLIP

**FairCLIP: Harnessing Fairness in Vision-Language Learning**
**å…¬å¹³CLIPï¼šåœ¨è§†è§‰-è¯­è¨€å­¦ä¹ ä¸­åˆ©ç”¨å…¬å¹³æ€§**

- Paper: https://arxiv.org/abs/2403.19949
- Code: https://github.com/Harvard-Ophthalmology-AI-Lab/FairCLIP

<a name="MAE"></a>

# MAE

<a name="Embodied-AI"></a>

# Embodied AI

**EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI**
**å…·èº«æ‰«æï¼šé¢å‘å…·èº«äººå·¥æ™ºèƒ½çš„å…¨æ–¹ä½å¤šæ¨¡æ€3Dæ„ŸçŸ¥å¥—ä»¶**

- Homepage: https://tai-wang.github.io/embodiedscan/
- Paper: https://arxiv.org/abs/2312.16170
- Code: https://github.com/OpenRobotLab/EmbodiedScan

**MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**
**MP5ï¼šé€šè¿‡ä¸»åŠ¨æ„ŸçŸ¥åœ¨Minecraftä¸­çš„å¤šæ¨¡æ€å¼€æ”¾å¼å…·èº«ç³»ç»Ÿ**

- Homepage: https://iranqin.github.io/MP5.github.io/ 
- Paper: https://arxiv.org/abs/2312.07472
- Code: https://github.com/IranQin/MP5

**LEMON: Learning 3D Human-Object Interaction Relation from 2D Images**
**æŸ æª¬ï¼šä»äºŒç»´å›¾åƒä¸­å­¦ä¹ 3Däºº-ç‰©äº¤äº’å…³ç³»**

- Paper: https://arxiv.org/abs/2312.08963
- Code: https://github.com/yyvhang/lemon_3d 

<a name="GAN"></a>

# GAN

<a name="OCR"></a>

# OCR

**An Empirical Study of Scaling Law for OCR**
**OCRç¼©æ”¾å®šå¾‹çš„å®è¯ç ”ç©¶**

- Paper: https://arxiv.org/abs/2401.00028
- Code: https://github.com/large-ocr-model/large-ocr-model.github.io

**ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting**
**ODMï¼šä¸€ç§ç”¨äºåœºæ™¯æ–‡æœ¬æ£€æµ‹å’Œå®šä½çš„æ–‡æœ¬-å›¾åƒè¿›ä¸€æ­¥å¯¹é½é¢„è®­ç»ƒæ–¹æ³•**

- Paper: https://arxiv.org/abs/2403.00303
- Code: https://github.com/PriNing/ODM 

<a name="NeRF"></a>

# NeRF

**PIE-NeRFğŸ•: Physics-based Interactive Elastodynamics with NeRF**
**PIE-NeRFğŸ•ï¼šåŸºäºç‰©ç†çš„äº¤äº’å¼å¼¹æ€§åŠ¨åŠ›å­¦ä¸NeRF**

- Paper: https://arxiv.org/abs/2311.13099
- Code: https://github.com/FYTalon/pienerf/ 

<a name="DETR"></a>

# DETR

**DETRs Beat YOLOs on Real-time Object Detection**
**DETRåœ¨å®æ—¶ç›®æ ‡æ£€æµ‹ä¸Šå‡»è´¥äº†YOLOs**

- Paper: https://arxiv.org/abs/2304.08069
- Code: https://github.com/lyuwenyu/RT-DETR

**Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement**
**æ˜¾è‘—æ€§DETRï¼šé€šè¿‡å±‚æ¬¡æ˜¾è‘—æ€§è¿‡æ»¤ç²¾ç‚¼å¢å¼ºæ£€æµ‹Transformer**

- Paper: https://arxiv.org/abs/2403.16131
- Code: https://github.com/xiuqhou/Salience-DETR

<a name="Prompt"></a>

# Prompt

<a name="MLLM"></a>

# å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)

**mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration**
**mPLUG-Owl2ï¼šé€šè¿‡æ¨¡æ€åä½œé©æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹**

- Paper: https://arxiv.org/abs/2311.04257
- Code: https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2

**Link-Context Learning for Multimodal LLMs**
**å¤šæ¨¡æ€LLMçš„é“¾æ¥ä¸Šä¸‹æ–‡å­¦ä¹ **

- Paper: https://arxiv.org/abs/2308.07891
- Code: https://github.com/isekai-portal/Link-Context-Learning/tree/main 

**OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation**
**OPERAï¼šé€šè¿‡è¿‡åº¦ä¿¡ä»»æƒ©ç½šå’Œåæ€-åˆ†é…ç¼“è§£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰**

- Paper: https://arxiv.org/abs/2311.17911
- Code: https://github.com/shikiw/OPERA

**Making Large Multimodal Models Understand Arbitrary Visual Prompts**
**åˆ¶ä½œèƒ½å¤Ÿç†è§£ä»»æ„è§†è§‰æç¤ºçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹**

- Homepage: https://vip-llava.github.io/ 
- Paper: https://arxiv.org/abs/2312.00784

**Pink: Unveiling the power of referential comprehension for multi-modal llms**
**ç²‰çº¢è‰²ï¼šæ­ç¤ºå¤šæ¨¡æ€LLMsä¸­å‚ç…§ç†è§£çš„åŠ›é‡**

- Paper: https://arxiv.org/abs/2310.00582
- Code: https://github.com/SY-Xuan/Pink

**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**
**Chat-UniViï¼šç»Ÿä¸€è§†è§‰è¡¨ç¤ºé€šè¿‡å›¾åƒå’Œè§†é¢‘ç†è§£èµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹**

- Paper: https://arxiv.org/abs/2311.08046
- Code: https://github.com/PKU-YuanGroup/Chat-UniVi

**OneLLM: One Framework to Align All Modalities with Language**
**OneLLMï¼šä¸€ä¸ªæ¡†æ¶ï¼Œå°†æ‰€æœ‰æ¨¡æ€ä¸è¯­è¨€å¯¹é½**

- Paper: https://arxiv.org/abs/2312.03700
- Code: https://github.com/csuhan/OneLLM

<a name="LLM"></a>

# å¤§è¯­è¨€æ¨¡å‹(LLM)

**VTimeLLM: Empower LLM to Grasp Video Moments**
**VTimeLLMï¼šèµ‹äºˆLLMæŠŠæ¡è§†é¢‘ç¬é—´çš„èƒ½åŠ›**

- Paper: https://arxiv.org/abs/2311.18445
- Code: https://github.com/huangb23/VTimeLLM 

<a name="NAS"></a>

# NAS

<a name="ReID"></a>

# ReID(é‡è¯†åˆ«)

**Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification**
**é­”æ³•ä»¤ç‰Œï¼šä¸ºå¤šæ¨¡æ€ç‰©ä½“é‡è¯†åˆ«é€‰æ‹©å¤šæ ·åŒ–çš„ä»¤ç‰Œ**

- Paper: https://arxiv.org/abs/2403.10254
- Code: https://github.com/924973292/EDITOR 

**Noisy-Correspondence Learning for Text-to-Image Person Re-identification**
**æ–‡æœ¬åˆ°å›¾åƒäººç‰©é‡è¯†åˆ«çš„å™ªå£°å¯¹åº”å­¦ä¹ **

- Paper: https://arxiv.org/abs/2308.09911

- Code : https://github.com/QinYang79/RDE 

<a name="Diffusion"></a>

# æ‰©æ•£æ¨¡å‹(Diffusion Models)

**InstanceDiffusion: Instance-level Control for Image Generation**
**å®ä¾‹æ‰©æ•£ï¼šå›¾åƒç”Ÿæˆä¸­çš„å®ä¾‹çº§æ§åˆ¶**

- Homepage: https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/

- Paper: https://arxiv.org/abs/2402.03290
- Code: https://github.com/frank-xwang/InstanceDiffusion

**Residual Denoising Diffusion Models**
**æ®‹å·®å»å™ªæ‰©æ•£æ¨¡å‹**

- Paper: https://arxiv.org/abs/2308.13712
- Code: https://github.com/nachifur/RDDM

**DeepCache: Accelerating Diffusion Models for Free**
**DeepCacheï¼šå…è´¹åŠ é€Ÿæ‰©æ•£æ¨¡å‹**

- Paper: https://arxiv.org/abs/2312.00858
- Code: https://github.com/horseee/DeepCache

**DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations**
**DEADiffï¼šä¸€ç§å…·æœ‰è§£è€¦è¡¨ç¤ºçš„é«˜æ•ˆé£æ ¼æ‰©æ•£æ¨¡å‹**

- Homepage: https://tianhao-qi.github.io/DEADiff/ 

- Paper: https://arxiv.org/abs/2403.06951
- Code: https://github.com/Tianhao-Qi/DEADiff_code

**SVGDreamer: Text Guided SVG Generation with Diffusion Model**
**SVGDreamerï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬å¼•å¯¼SVGç”Ÿæˆ**

- Paper: https://arxiv.org/abs/2312.16476
- Code: https://ximinng.github.io/SVGDreamer-project/

**InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model**
**äº¤äº’å¼æ‰©æ•£ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„äº¤äº’æ§åˆ¶**

- Paper: https://arxiv.org/abs/2312.05849
- Code: https://github.com/jiuntian/interactdiffusion

**MMA-Diffusion: MultiModal Attack on Diffusion Models**
**MMA-Diffusionï¼šå¯¹æ‰©æ•£æ¨¡å‹çš„è·¨æ¨¡æ€æ”»å‡»**

- Paper: https://arxiv.org/abs/2311.17516
- Code: https://github.com/yangyijune/MMA-Diffusion

**VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models**
**è§†é¢‘è¿åŠ¨å®šåˆ¶ï¼šåˆ©ç”¨æ—¶é—´æ³¨æ„åŠ›è‡ªé€‚åº”çš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹**

- Homeoage: https://video-motion-customization.github.io/ 
- Paper: https://arxiv.org/abs/2312.00845
- Code: https://github.com/HyeonHo99/Video-Motion-Customization

<a name="Vision-Transformer"></a>

# Vision Transformer

**TransNeXt: Robust Foveal Visual Perception for Vision Transformers**
**TransNeXtï¼šä¸ºè§†è§‰Transformeræä¾›é²æ£’çš„é»„æ–‘è§†è§‰æ„ŸçŸ¥**

- Paper: https://arxiv.org/abs/2311.17132
- Code: https://github.com/DaiShiResearch/TransNeXt

**RepViT: Revisiting Mobile CNN From ViT Perspective**
**RepViTï¼šä»ViTè§†è§’é‡æ–°å®¡è§†ç§»åŠ¨CNN**

- Paper: https://arxiv.org/abs/2307.09283
- Code: https://github.com/THU-MIG/RepViT

**A General and Efficient Training for Transformer via Token Expansion**
**é€šè¿‡è¯å…ƒæ‰©å±•è¿›è¡Œé€šç”¨ä¸”é«˜æ•ˆçš„Transformerè®­ç»ƒ**

- Paper: https://arxiv.org/abs/2404.00672
- Code: https://github.com/Osilly/TokenExpansion 

<a name="VL"></a>

# è§†è§‰å’Œè¯­è¨€(Vision-Language)

**PromptKD: Unsupervised Prompt Distillation for Vision-Language Models**
**æç¤ºKDï¼šç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ— ç›‘ç£æç¤ºè’¸é¦**

- Paper: https://arxiv.org/abs/2403.02781
- Code: https://github.com/zhengli97/PromptKD

**FairCLIP: Harnessing Fairness in Vision-Language Learning**
**å…¬å¹³CLIPï¼šåœ¨è§†è§‰è¯­è¨€å­¦ä¹ ä¸­åˆ©ç”¨å…¬å¹³æ€§**

- Paper: https://arxiv.org/abs/2403.19949
- Code: https://github.com/Harvard-Ophthalmology-AI-Lab/FairCLIP

<a name="Object-Detection"></a>

# ç›®æ ‡æ£€æµ‹(Object Detection)

**DETRs Beat YOLOs on Real-time Object Detection**
**DETRsåœ¨å®æ—¶ç›®æ ‡æ£€æµ‹æ–¹é¢å‡»è´¥äº†YOLOs**

- Paper: https://arxiv.org/abs/2304.08069
- Code: https://github.com/lyuwenyu/RT-DETR

**Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation**
**åˆ©ç”¨é›¶æ ·æœ¬æ—¥å¤œé—´åŸŸé€‚åº”å¢å¼ºç›®æ ‡æ£€æµ‹**

- Paper: https://arxiv.org/abs/2312.01220
- Code: https://github.com/ZPDu/Boosting-Object-Detection-with-Zero-Shot-Day-Night-Domain-Adaptation 

**YOLO-World: Real-Time Open-Vocabulary Object Detection**
**YOLO-Worldï¼šå®æ—¶å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹**

- Paper: https://arxiv.org/abs/2401.17270
- Code: https://github.com/AILab-CVC/YOLO-World

**Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement**
**æ˜¾è‘—æ€§DETRï¼šé€šè¿‡åˆ†å±‚æ˜¾è‘—æ€§æ»¤æ³¢ä¼˜åŒ–æå‡æ£€æµ‹Transformer**

- Paper: https://arxiv.org/abs/2403.16131
- Code: https://github.com/xiuqhou/Salience-DETR

<a name="Anomaly-Detection"></a>

# å¼‚å¸¸æ£€æµ‹(Anomaly Detection)

**Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection**
**å¼€æ”¾é›†ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸­çš„å¼‚å¸¸å¼‚è´¨æ€§å­¦ä¹ **

- Paper: https://arxiv.org/abs/2310.12790
- Code: https://github.com/mala-lab/AHL

<a name="VT"></a>

# ç›®æ ‡è·Ÿè¸ª(Object Tracking)

**Delving into the Trajectory Long-tail Distribution for Muti-object Tracking**
**æ·±å…¥æ¢ç©¶å¤šç›®æ ‡è·Ÿè¸ªä¸­çš„è½¨è¿¹é•¿å°¾åˆ†å¸ƒ**

- Paper: https://arxiv.org/abs/2403.04700
- Code: https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT 

<a name="Semantic-Segmentation"></a>

# è¯­ä¹‰åˆ†å‰²(Semantic Segmentation)

**Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation**
**æ›´å¼ºã€æ›´å°‘ã€æ›´ä¼˜è¶Šï¼šåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å®ç°é¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²**

- Paper: https://arxiv.org/abs/2312.04265
- Code: https://github.com/w1oves/Rein

**SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation**
**å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„ç®€å•ç¼–ç å™¨-è§£ç å™¨ï¼šSED**

- Paper: https://arxiv.org/abs/2311.15537
- Code: https://github.com/xb534/SED 

<a name="MI"></a>

# åŒ»å­¦å›¾åƒ(Medical Image)

**Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology**
**ç‰¹å¾å†åµŒå…¥ï¼šè¿ˆå‘è®¡ç®—ç—…ç†å­¦åŸºç¡€æ¨¡å‹çº§åˆ«çš„æ€§èƒ½**

- Paper: https://arxiv.org/abs/2402.17228
- Code: https://github.com/DearCaat/RRT-MIL

**VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis**
**VoCoï¼šä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„3DåŒ»å­¦å›¾åƒåˆ†æä½“ç§¯å¯¹æ¯”å­¦ä¹ æ¡†æ¶**

- Paper: https://arxiv.org/abs/2402.17300
- Code: https://github.com/Luffy03/VoCo

**ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images**
**ChAda-ViTï¼šå¼‚æ„æ˜¾å¾®é•œå›¾åƒè”åˆè¡¨ç¤ºå­¦ä¹ çš„é€šé“è‡ªé€‚åº”æ³¨æ„åŠ›**

- Paper: https://arxiv.org/abs/2311.15264
- Code: https://github.com/nicoboou/chada_vit 

<a name="MIS"></a>

# åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation)



<a name="Autonomous-Driving"></a>

# è‡ªåŠ¨é©¾é©¶(Autonomous Driving)

**UniPAD: A Universal Pre-training Paradigm for Autonomous Driving**
**UniPADï¼šè‡ªåŠ¨é©¾é©¶çš„é€šç”¨é¢„è®­ç»ƒèŒƒå¼**

- Paper: https://arxiv.org/abs/2310.08370
- Code: https://github.com/Nightmare-n/UniPAD

**Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications**
**Cam4DOccï¼šè‡ªåŠ¨é©¾é©¶åº”ç”¨ä¸­ä»…ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œ4Då ç”¨é¢„æµ‹çš„åŸºå‡†æµ‹è¯•**

- Paper: https://arxiv.org/abs/2311.17663
- Code: https://github.com/haomo-ai/Cam4DOcc

**Memory-based Adapters for Online 3D Scene Perception**
**åŸºäºå†…å­˜çš„åœ¨çº¿3Dåœºæ™¯æ„ŸçŸ¥é€‚é…å™¨**

- Paper: https://arxiv.org/abs/2403.06974
- Code: https://github.com/xuxw98/Online3D

**Symphonize 3D Semantic Scene Completion with Contextual Instance Queries**
**å°†3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ä¸ä¸Šä¸‹æ–‡å®ä¾‹æŸ¥è¯¢åŒæ­¥åŒ–**

- Paper: https://arxiv.org/abs/2306.15670
- Code: https://github.com/hustvl/Symphonies

**A Real-world Large-scale Dataset for Roadside Cooperative Perception**
**çœŸå®ä¸–ç•Œå¤§è§„æ¨¡é“è·¯ä¾§ååŒæ„ŸçŸ¥æ•°æ®é›†**

- Paper: https://arxiv.org/abs/2403.10145
- Code: https://github.com/AIR-THU/DAIR-RCooper

**Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving**
**å•è§†å’Œå¤šè§†æ·±åº¦è‡ªé€‚åº”èåˆç”¨äºè‡ªåŠ¨é©¾é©¶**

- Paper: https://arxiv.org/abs/2403.07535
- Code: https://github.com/Junda24/AFNet

**Traffic Scene Parsing through the TSP6K Dataset**
**é€šè¿‡TSP6Kæ•°æ®é›†è¿›è¡Œäº¤é€šåœºæ™¯è§£æ**

- Paper: https://arxiv.org/pdf/2303.02835.pdf
- Code: https://github.com/PengtaoJiang/TSP6K 

<a name="3D-Point-Cloud"></a>

# 3Dç‚¹äº‘(3D-Point-Cloud)



<a name="3DOD"></a>

# 3Dç›®æ ‡æ£€æµ‹(3D Object Detection)

**PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection**
**PTTï¼šé«˜æ•ˆæ—¶åº3Dç›®æ ‡æ£€æµ‹çš„ç‚¹-è½¨è¿¹å˜æ¢å™¨**

- Paper: https://arxiv.org/abs/2312.08371
- Code: https://github.com/kuanchihhuang/PTT

**UniMODE: Unified Monocular 3D Object Detection**
**UniMODEï¼šç»Ÿä¸€å•ç›®3Dç›®æ ‡æ£€æµ‹**

- Paper: https://arxiv.org/abs/2402.18573

<a name="3DOD"></a>

# 3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation)

<a name="Image-Editing"></a>

# å›¾åƒç¼–è¾‘(Image Editing)

**Edit One for All: Interactive Batch Image Editing**
**ä¸€é”®ç¼–è¾‘ï¼šäº¤äº’å¼æ‰¹é‡å›¾åƒç¼–è¾‘**

- Homepage: https://thaoshibe.github.io/edit-one-for-all 
- Paper: https://arxiv.org/abs/2401.10219
- Code: https://github.com/thaoshibe/edit-one-for-all

<a name="Video-Editing"></a>

# è§†é¢‘ç¼–è¾‘(Video Editing)

**MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers**
**MaskINTï¼šé€šè¿‡æ’å€¼éè‡ªå›å½’æ©ç å˜æ¢å™¨è¿›è¡Œè§†é¢‘ç¼–è¾‘**

- Homepage:  [https://maskint.github.io](https://maskint.github.io/) 

- Paper: https://arxiv.org/abs/2312.12468

<a name="LLV"></a>

# Low-level Vision

**Residual Denoising Diffusion Models**
**æ®‹å·®å»å™ªæ‰©æ•£æ¨¡å‹**

- Paper: https://arxiv.org/abs/2308.13712
- Code: https://github.com/nachifur/RDDM

**Boosting Image Restoration via Priors from Pre-trained Models**
**é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹å…ˆéªŒä¿¡æ¯å¢å¼ºå›¾åƒæ¢å¤**

- Paper: https://arxiv.org/abs/2403.06793

<a name="SR"></a>

# è¶…åˆ†è¾¨ç‡(Super-Resolution)

**SeD: Semantic-Aware Discriminator for Image Super-Resolution**
**SeDï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„è¯­ä¹‰æ„ŸçŸ¥åˆ¤åˆ«å™¨**

- Paper: https://arxiv.org/abs/2402.19387
- Code: https://github.com/lbc12345/SeD

**APISR: Anime Production Inspired Real-World Anime Super-Resolution**
**APISRï¼šå—åŠ¨ç”»åˆ¶ä½œå¯å‘çš„ç°å®ä¸–ç•ŒåŠ¨ç”»è¶…åˆ†è¾¨ç‡**

- Paper: https://arxiv.org/abs/2403.01598
- Code: https://github.com/Kiteretsu77/APISR 

<a name="Denoising"></a>

# å»å™ª(Denoising)

## å›¾åƒå»å™ª(Image Denoising)

<a name="3D-Human-Pose-Estimation"></a>

# 3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation)

**Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation**
**æ²™æ¼åˆ†è¯å™¨ç”¨äºé«˜æ•ˆåŸºäºTransformerçš„3Däººä½“å§¿æ€ä¼°è®¡**

- Paper: https://arxiv.org/abs/2311.12028
- Code: https://github.com/NationalGAILab/HoT 

<a name="Image-Generation"></a>

# å›¾åƒç”Ÿæˆ(Image Generation)

**InstanceDiffusion: Instance-level Control for Image Generation**
**å®ä¾‹æ‰©æ•£ï¼šå›¾åƒç”Ÿæˆä¸­çš„å®ä¾‹çº§æ§åˆ¶**

- Homepage: https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/

- Paper: https://arxiv.org/abs/2402.03290
- Code: https://github.com/frank-xwang/InstanceDiffusion

**ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations**
**ECLIPSEï¼šä¸€ç§é«˜æ•ˆåˆ©ç”¨èµ„æºçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå…ˆéªŒ**

- Homepage: https://eclipse-t2i.vercel.app/
- Paper: https://arxiv.org/abs/2312.04655

- Code: https://github.com/eclipse-t2i/eclipse-inference

**Instruct-Imagen: Image Generation with Multi-modal Instruction**
**æŒ‡ä»¤-å›¾åƒï¼šå¤šæ¨¡æ€æŒ‡ä»¤ä¸‹çš„å›¾åƒç”Ÿæˆ**

- Paper: https://arxiv.org/abs/2401.01952

**Residual Denoising Diffusion Models**
**æ®‹å·®å»å™ªæ‰©æ•£æ¨¡å‹**

- Paper: https://arxiv.org/abs/2308.13712
- Code: https://github.com/nachifur/RDDM

**UniGS: Unified Representation for Image Generation and Segmentation**
**UniGSï¼šå›¾åƒç”Ÿæˆä¸åˆ†å‰²çš„ç»Ÿä¸€è¡¨ç¤º**

- Paper: https://arxiv.org/abs/2312.01985

**Multi-Instance Generation Controller for Text-to-Image Synthesis**
**å¤šå®ä¾‹ç”Ÿæˆæ§åˆ¶å™¨ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒåˆæˆ**

- Paper: https://arxiv.org/abs/2402.05408
- Code: https://github.com/limuloo/migc

**SVGDreamer: Text Guided SVG Generation with Diffusion Model**
**SVGDreamerï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬å¼•å¯¼SVGç”Ÿæˆ**

- Paper: https://arxiv.org/abs/2312.16476
- Code: https://ximinng.github.io/SVGDreamer-project/

**InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model**
**äº¤äº’æ‰©æ•£ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„äº¤äº’æ§åˆ¶**

- Paper: https://arxiv.org/abs/2312.05849
- Code: https://github.com/jiuntian/interactdiffusion

**Ranni: Taming Text-to-Image Diffusion for Accurate Prompt Following**
**Ranniï¼šé©¯æœæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ï¼Œå®ç°å‡†ç¡®æç¤ºè·Ÿéš**

- Paper: https://arxiv.org/abs/2311.17002
- Code: https://github.com/ali-vilab/Ranni

<a name="Video-Generation"></a>

# è§†é¢‘ç”Ÿæˆ(Video Generation)

**Vlogger: Make Your Dream A Vlog**
**è§†é¢‘åšä¸»ï¼šè®©ä½ çš„æ¢¦æƒ³æˆä¸ºä¸€æ¡£è§†é¢‘åšå®¢**

- Paper: https://arxiv.org/abs/2401.09414
- Code: https://github.com/Vchitect/Vlogger

**VBench: Comprehensive Benchmark Suite for Video Generative Models**
**VBenchï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å…¨é¢åŸºå‡†æµ‹è¯•å¥—ä»¶**

- Homepage: https://vchitect.github.io/VBench-project/ 
- Paper: https://arxiv.org/abs/2311.17982
- Code: https://github.com/Vchitect/VBench

**VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models**
**è§†é¢‘è¿åŠ¨å®šåˆ¶ï¼šåˆ©ç”¨æ—¶é—´æ³¨æ„åŠ›è‡ªé€‚åº”çš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹**

- Homeoage: https://video-motion-customization.github.io/ 
- Paper: https://arxiv.org/abs/2312.00845
- Code: https://github.com/HyeonHo99/Video-Motion-Customization

<a name="3D-Generation"></a>

# 3Dç”Ÿæˆ

**CityDreamer: Compositional Generative Model of Unbounded 3D Cities**
**åŸå¸‚æ¢¦æƒ³å®¶ï¼šæ— é™3DåŸå¸‚çš„æ„å›¾ç”Ÿæˆæ¨¡å‹**

- Homepage: https://haozhexie.com/project/city-dreamer/ 
- Paper: https://arxiv.org/abs/2309.00610
- Code: https://github.com/hzxie/city-dreamer

**LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching**
**æ¸…é†’æ¢¦å¢ƒè€…ï¼šé€šè¿‡åŒºé—´å¾—åˆ†åŒ¹é…å®ç°é«˜ä¿çœŸæ–‡æœ¬åˆ°3Dç”Ÿæˆ**

- Paper: https://arxiv.org/abs/2311.11284
- Code: https://github.com/EnVision-Research/LucidDreamer 

<a name="Video-Understanding"></a>

# è§†é¢‘ç†è§£(Video Understanding)

**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**
**MVBenchï¼šä¸€ä¸ªå…¨é¢çš„è·¨æ¨¡æ€è§†é¢‘ç†è§£åŸºå‡†**

- Paper: https://arxiv.org/abs/2311.17005
- Code: https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2 

<a name="KD"></a>

# çŸ¥è¯†è’¸é¦(Knowledge Distillation)

**Logit Standardization in Knowledge Distillation**
**çŸ¥è¯†è’¸é¦ä¸­çš„Logitæ ‡å‡†åŒ–**

- Paper: https://arxiv.org/abs/2403.01427
- Code: https://github.com/sunshangquan/logit-standardization-KD

**Efficient Dataset Distillation via Minimax Diffusion**
**é€šè¿‡æœ€å°-æœ€å¤§æ‰©æ•£è¿›è¡Œé«˜æ•ˆæ•°æ®é›†è’¸é¦**

- Paper: https://arxiv.org/abs/2311.15529
- Code: https://github.com/vimar-gu/MinimaxDiffusion

<a name="Stereo-Matching"></a>

# ç«‹ä½“åŒ¹é…(Stereo Matching)

**Neural Markov Random Field for Stereo Matching**
**ç¥ç»é©¬å°”å¯å¤«éšæœºåœºç”¨äºç«‹ä½“åŒ¹é…**

- Paper: https://arxiv.org/abs/2403.11193
- Code: https://github.com/aeolusguan/NMRF 

<a name="SGG"></a>

# åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation)

**HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation**
**HiKER-SGGï¼šå±‚æ¬¡çŸ¥è¯†å¢å¼ºé²æ£’åœºæ™¯å›¾ç”Ÿæˆ**

- Homepage: https://zhangce01.github.io/HiKER-SGG/ 
- Paper : https://arxiv.org/abs/2403.12033
- Code: https://github.com/zhangce01/HiKER-SGG

<a name="Video-Quality-Assessment"></a>

# è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment)

**KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos**
**KVQï¼šçŸ­è§†é¢‘çš„ä¸‡èŠ±ç­’è§†é¢‘è´¨é‡è¯„ä¼°**

- Homepage: https://lixinustc.github.io/projects/KVQ/ 

- Paper: https://arxiv.org/abs/2402.07220
- Code: https://github.com/lixinustc/KVQ-Challenge-CVPR-NTIRE2024

<a name="Datasets"></a>

# æ•°æ®é›†(Datasets)

**A Real-world Large-scale Dataset for Roadside Cooperative Perception**
**ç°å®ä¸–ç•Œå¤§è§„æ¨¡é“è·¯ä¾§ååŒæ„ŸçŸ¥æ•°æ®é›†**

- Paper: https://arxiv.org/abs/2403.10145
- Code: https://github.com/AIR-THU/DAIR-RCooper

**Traffic Scene Parsing through the TSP6K Dataset**
**é€šè¿‡TSP6Kæ•°æ®é›†è¿›è¡Œäº¤é€šåœºæ™¯è§£æ**

- Paper: https://arxiv.org/pdf/2303.02835.pdf
- Code: https://github.com/PengtaoJiang/TSP6K 

<a name="Others"></a>

# å…¶ä»–(Others)

**Object Recognition as Next Token Prediction**
**å¯¹è±¡è¯†åˆ«ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹**

- Paper: https://arxiv.org/abs/2312.02142
- Code: https://github.com/kaiyuyue/nxtp

**ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks**
**ParameterNetï¼šå‚æ•°å³æ˜¯æ‰€æœ‰ï¼Œç”¨äºç§»åŠ¨ç½‘ç»œå¤§è§„æ¨¡è§†è§‰é¢„è®­ç»ƒ**

- Paper: https://arxiv.org/abs/2306.14525
- Code: https://parameternet.github.io/ 

**Seamless Human Motion Composition with Blended Positional Encodings**
**æ— ç¼çš„äººä½“è¿åŠ¨åˆæˆä¸æ··åˆä½ç½®ç¼–ç **

- Paper: https://arxiv.org/abs/2402.15509
- Code: https://github.com/BarqueroGerman/FlowMDM 

**LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning**
**LL3DAï¼šç”¨äºå…¨3Dç†è§£ã€æ¨ç†å’Œè§„åˆ’çš„è§†è§‰äº¤äº’å¼æŒ‡ä»¤è°ƒä¼˜**

- Homepage:  https://ll3da.github.io/ 

- Paper: https://arxiv.org/abs/2311.18651
- Code: https://github.com/Open3DA/LL3DA

 **CLOVA: A Closed-LOop Visual Assistant with Tool Usage and Update**

- Homepage: https://clova-tool.github.io/ 
- Paper: https://arxiv.org/abs/2312.10908

**MoMask: Generative Masked Modeling of 3D Human Motions**
**MoMaskï¼š3Däººä½“åŠ¨ä½œçš„ç”Ÿæˆå¼æ©ç å»ºæ¨¡**

- Paper: https://arxiv.org/abs/2312.00063
- Code: https://github.com/EricGuo5513/momask-codes

 **Amodal Ground Truth and Completion in the Wild**

- Homepage: https://www.robots.ox.ac.uk/~vgg/research/amodal/ 
- Paper: https://arxiv.org/abs/2312.17247
- Code: https://github.com/Championchess/Amodal-Completion-in-the-Wild

**Improved Visual Grounding through Self-Consistent Explanations**
**é€šè¿‡è‡ªæ´½è§£é‡Šæå‡è§†è§‰å®šä½**

- Paper: https://arxiv.org/abs/2312.04554
- Code: https://github.com/uvavision/SelfEQ

**ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object**
**ImageNet-Dï¼šåœ¨æ‰©æ•£åˆæˆç‰©ä½“ä¸ŠåŸºå‡†æµ‹è¯•ç¥ç»ç½‘ç»œé²æ£’æ€§**

- Homepage: https://chenshuang-zhang.github.io/imagenet_d/
- Paper: https://arxiv.org/abs/2403.18775
- Code: https://github.com/chenshuang-zhang/imagenet_d

**Learning from Synthetic Human Group Activities**
**ä»åˆæˆäººç±»ç¾¤ä½“æ´»åŠ¨ä¸­å­¦ä¹ **

- Homepage: https://cjerry1243.github.io/M3Act/ 
- Paper  https://arxiv.org/abs/2306.16772
- Code: https://github.com/cjerry1243/M3Act

**A Cross-Subject Brain Decoding Framework**
**è·¨å­¦ç§‘å¤§è„‘è§£ç æ¡†æ¶**

- Homepage: https://littlepure2333.github.io/MindBridge/
- Paper: https://arxiv.org/abs/2404.07850
- Code: https://github.com/littlepure2333/MindBridge

**Multi-Task Dense Prediction via Mixture of Low-Rank Experts**
**é€šè¿‡ä½ç§©ä¸“å®¶æ··åˆçš„å¤šä»»åŠ¡å¯†é›†é¢„æµ‹**

- Paper : https://arxiv.org/abs/2403.17749
- Code: https://github.com/YuqiYang213/MLoRE

**Contrastive Mean-Shift Learning for Generalized Category Discovery**
**å¯¹æ¯”å‡å€¼æ¼‚ç§»å­¦ä¹ ç”¨äºå¹¿ä¹‰ç±»åˆ«å‘ç°**

- Homepage: https://postech-cvlab.github.io/cms/ 
- Paper: https://arxiv.org/abs/2404.09451
- Code: https://github.com/sua-choi/CMS
  
